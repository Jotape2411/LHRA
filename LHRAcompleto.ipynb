{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMlNhjluvzDxELdy+TYMvb9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"6JMO7thIeBAT"},"outputs":[],"source":["# lhra_core/agent.py\n","# ==============================================================\n","# Versão: v5.4.3.7_plus_final_enhanced3_full_multiEnv_genePool\n","# Autor:  OpenAI Assistant (chatGPT)\n","# ==============================================================\n","import os\n","import time\n","import json\n","import math\n","import copy\n","import queue\n","import random\n","import pathlib\n","import typing as tp\n","from collections import deque, defaultdict\n","from lhra_core.kernel_manager import KernelManager\n","from lhra_core.focus_manager  import FocusManager\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.nn import functional as F\n","from torch.utils.tensorboard import SummaryWriter\n","\n","import gymnasium as gym\n","\n","# ----------------------------------------------------------------\n","# CONFIGURAÇÕES GERAIS\n","# ----------------------------------------------------------------\n","DEVICE: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","DEFAULT_CONFIG = {\n","    \"env_list\": [\"CartPole-v1\", \"MountainCar-v0\"],\n","    \"gamma\": 0.99,\n","    \"entropy_coef\": 0.005,\n","    \"lr\": 3e-4,\n","    \"lr_world\": 3e-4,\n","    \"lr_min\": 1e-5,\n","    \"rollback_ratio\": 0.7,\n","    \"rollback_world_loss_threshold\": 0.15,\n","    \"rollback_cooldown\": 500,\n","    \"auto_expand_threshold\": 1.2,\n","    \"prune_threshold\": 0.8,\n","    \"cooldown_steps\": 500,\n","    \"gene_pool_interval\": 1000,\n","    \"gene_evaluation_steps\": 600,\n","    \"max_episode_steps\": 500,\n","    \"world_loss_history_len\": 30,\n","    \"tensorboard_dir\": \"./runs/lhra\",\n","}\n","\n","# ----------------------------------------------------------------\n","# REDE BÁSICA DA POLÍTICA\n","# ----------------------------------------------------------------\n","class BasePolicy(nn.Module):\n","    def __init__(self, obs_dim: int, act_dim: int):\n","        super().__init__()\n","        self.fc1 = nn.Linear(obs_dim, 128)\n","        self.fc2 = nn.Linear(128, 128)\n","        self.pi = nn.Linear(128, act_dim)\n","        self.v  = nn.Linear(128, 1)\n","        # blocos extras dinâmicos\n","        self.extra_blocks: nn.ModuleDict = nn.ModuleDict()\n","\n","    def forward(self, x: torch.Tensor):\n","        x = F.relu(self.fc1(x))\n","        for block in self.extra_blocks.values():\n","            x = block(x)\n","        x = F.relu(self.fc2(x))\n","        logits = self.pi(x)\n","        value  = self.v(x)\n","        return logits, value.squeeze(-1)\n","\n","    # ------- APIs dinâmicas ---------\n","    def add_extra_block(self, name: str, block: nn.Module):\n","        self.extra_blocks[name] = block.to(DEVICE)\n","\n","    def remove_extra_block(self, name: str):\n","        if name in self.extra_blocks:\n","            self.extra_blocks.pop(name)\n","\n","# ----------------------------------------------------------------\n","# META‑CONTROLLER (6 métricas dinamicamente ponderadas)\n","# ----------------------------------------------------------------\n","class DynamicWeightAdapter(nn.Module):\n","    \"\"\"\n","    Combine 6 sinais:\n","    0: chaos_loss\n","    1: organization_loss\n","    2: memory_loss\n","    3: policy_loss\n","    4: world_loss\n","    5: stability_score\n","    \"\"\"\n","    def __init__(self):\n","        super().__init__()\n","        # inicializa pesos iguais\n","        self.weights = nn.Parameter(torch.ones(6) / 6)\n","\n","    def forward(self, losses: torch.Tensor) -> torch.Tensor:\n","        # softmax p/ manter soma=1\n","        w = torch.softmax(self.weights, dim=0)\n","        return (w * losses).sum()\n","\n","# ----------------------------------------------------------------\n","# WORLD MODEL (pequeno modelo preditivo)\n","# ----------------------------------------------------------------\n","class WorldModel(nn.Module):\n","    def __init__(self, obs_dim: int, act_dim: int):\n","        super().__init__()\n","        self.trunk = nn.Sequential(\n","            nn.Linear(obs_dim + act_dim, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 128),\n","            nn.ReLU(),\n","        )\n","        self.delta = nn.Linear(128, obs_dim)  # prever ∆estado\n","        self.reward = nn.Linear(128, 1)       # prever reward\n","\n","    def forward(self, obs: torch.Tensor, act_onehot: torch.Tensor):\n","        x = torch.cat([obs, act_onehot], dim=-1)\n","        h = self.trunk(x)\n","        return self.delta(h), self.reward(h).squeeze(-1)\n","\n","# ----------------------------------------------------------------\n","# AUTO‑EXPAND + PRUNE MANAGER (hook simplificado)\n","# ----------------------------------------------------------------\n","class AutoExpandPruneManager:\n","    def __init__(self, policy: BasePolicy, cfg: dict):\n","        self.policy = policy\n","        self.cfg = cfg\n","        self.last_change_step = 0\n","        self.block_id = 0\n","        self.block_stats: dict[str, float] = {}  # importância\n","\n","    # ----- importância = média(|grad|) + média(|output|)\n","    def _compute_block_importance(self, block_name: str, dummy_input: torch.Tensor):\n","        block = self.policy.extra_blocks[block_name]\n","        block.zero_grad(set_to_none=True)\n","        out = block(dummy_input).mean()\n","        out.backward()\n","        grad_norm = 0.0\n","        for p in block.parameters():\n","            if p.grad is not None:\n","                grad_norm += p.grad.detach().abs().mean().item()\n","        out_val = out.detach().abs().item()\n","        return grad_norm + out_val\n","\n","    def maybe_expand_or_prune(self, step_idx: int, policy_loss_scalar: float):\n","        if step_idx - self.last_change_step < self.cfg[\"cooldown_steps\"]:\n","            return\n","        # expand?\n","        if policy_loss_scalar > self.cfg[\"auto_expand_threshold\"]:\n","            name = f\"extra_{self.block_id}\"\n","            self.block_id += 1\n","            self.policy.add_extra_block(name, nn.Sequential(\n","                nn.Linear(128, 128),\n","                nn.ReLU(),\n","            ))\n","            print(f\"[AutoExpand] Adicionado {name}\")\n","            self.last_change_step = step_idx\n","        # prune?\n","        elif policy_loss_scalar < self.cfg[\"prune_threshold\"] and self.policy.extra_blocks:\n","            dummy = torch.randn(1, 128, device=DEVICE)\n","            # avalia importância de cada bloco\n","            for bn in self.policy.extra_blocks.keys():\n","                self.block_stats[bn] = self._compute_block_importance(bn, dummy)\n","            least = min(self.block_stats.items(), key=lambda kv: kv[1])[0]\n","            self.policy.remove_extra_block(least)\n","            print(f\"[AutoPrune] Removido bloco {least}\")\n","            self.last_change_step = step_idx\n","\n","# ----------------------------------------------------------------\n","# GENE POOL (simplificado; offline eval é realizada externamente)\n","# ----------------------------------------------------------------\n","class GenePool:\n","    def __init__(self, cfg: dict):\n","        self.cfg = cfg\n","        self.pool: list[dict] = []  # cada gene = dict(state_dict, meta_weights)\n","\n","    def add_gene(self, policy: BasePolicy, meta_ctrl: DynamicWeightAdapter):\n","        gene = {\n","            \"policy\": copy.deepcopy(policy.state_dict()),\n","            \"meta\": meta_ctrl.weights.detach().cpu().clone(),\n","            \"timestamp\": time.time(),\n","        }\n","        self.pool.append(gene)\n","\n","    def sample_best_gene(self) -> tp.Optional[dict]:\n","        # exemplo: retorna o gene mais recente\n","        return self.pool[-1] if self.pool else None\n","\n","# ----------------------------------------------------------------\n","# LHRA AGENT\n","# ----------------------------------------------------------------\n","class LHRA_Agent:\n","    def __init__(self, cfg: dict | None = None) -> None:\n","        self.cfg = cfg or DEFAULT_CONFIG\n","        self.envs = [gym.make(name) for name in self.cfg[\"env_list\"]]\n","        self.env_idx = 0\n","        self.env = self.envs[self.env_idx]\n","        obs_dim = np.prod(self.env.observation_space.shape)\n","        act_dim = self.env.action_space.n\n","\n","        # Redes\n","        self.policy = BasePolicy(obs_dim, act_dim).to(DEVICE)\n","        self.meta_controller = DynamicWeightAdapter().to(DEVICE)\n","        self.world_model = WorldModel(obs_dim, act_dim).to(DEVICE)\n","\n","        # Optimizadores\n","        self.opt_policy = optim.Adam(self.policy.parameters(), lr=self.cfg[\"lr\"])\n","        self.opt_world  = optim.Adam(self.world_model.parameters(), lr=self.cfg[\"lr_world\"])\n","\n","        # Scheduler do world model\n","        self.sched_world = optim.lr_scheduler.ReduceLROnPlateau(\n","            self.opt_world, mode=\"min\", factor=0.5, patience=100, min_lr=self.cfg[\"lr_min\"]\n","        )\n","\n","        # AutoExpand/Prune e GenePool\n","        self.auto_manager = AutoExpandPruneManager(self.policy, self.cfg)\n","        self.gene_pool = GenePool(self.cfg)\n","\n","        # Buffers\n","        self.rollout_obs:   list[np.ndarray] = []\n","        self.rollout_act:   list[int] = []\n","        self.rollout_rew:   list[float] = []\n","        self.rollout_next:  list[np.ndarray] = []\n","        self.rollout_done:  list[bool] = []\n","\n","        # World loss history para rollback parcial\n","        self.world_loss_hist = deque(maxlen=self.cfg[\"world_loss_history_len\"])\n","\n","        # Checkpoints\n","        self.best_reward = -float(\"inf\")\n","        self.last_rollback_step = -self.cfg[\"rollback_cooldown\"]\n","\n","        # Logs\n","        run_dir = pathlib.Path(self.cfg[\"tensorboard_dir\"])\n","        run_dir.mkdir(parents=True, exist_ok=True)\n","        self.writer = SummaryWriter(run_dir)\n","\n","        # Estado inicial\n","        self.state, _ = self.env.reset(seed=0)\n","\n","    # ------------------------------------------------------------\n","    # UTILITÁRIOS\n","    # ------------------------------------------------------------\n","    def _one_hot(self, a: int, n: int):\n","        vec = np.zeros(n, dtype=np.float32)\n","        vec[a] = 1.0\n","        return vec\n","\n","    def map_action_to_env(self, logits: torch.Tensor) -> int:\n","        probs = torch.softmax(logits, dim=-1).cpu().detach().numpy()\n","        return int(np.random.choice(len(probs), p=probs))\n","\n","    # ------------------------------------------------------------\n","    # TREINO DO WORLD MODEL\n","    # ------------------------------------------------------------\n","    def update_world_model(self, batch_size: int = 64):\n","        if len(self.rollout_obs) < batch_size:\n","            return 0.0\n","        idx = np.random.choice(len(self.rollout_obs), batch_size, replace=False)\n","        obs     = torch.tensor(np.array(self.rollout_obs)[idx]).float().to(DEVICE)\n","        acts    = torch.tensor(np.array(self.rollout_act)[idx]).long().to(DEVICE)\n","        nextobs = torch.tensor(np.array(self.rollout_next)[idx]).float().to(DEVICE)\n","        rews    = torch.tensor(np.array(self.rollout_rew)[idx]).float().to(DEVICE)\n","\n","        act_oh = torch.tensor([self._one_hot(a, self.env.action_space.n) for a in acts.cpu().numpy()]).to(DEVICE)\n","        pred_delta, pred_rew = self.world_model(obs, act_oh)\n","        true_delta = nextobs - obs\n","\n","        loss = F.mse_loss(pred_delta, true_delta) + F.mse_loss(pred_rew, rews)\n","        self.opt_world.zero_grad()\n","        loss.backward()\n","        self.opt_world.step()\n","        self.sched_world.step(loss.item())\n","        self.world_loss_hist.append(loss.item())\n","        return loss.item()\n","\n","    # ------------------------------------------------------------\n","    # POLÍTICA (A2C simplificado)\n","    # ------------------------------------------------------------\n","    def update_policy(self, gamma: float = 0.99):\n","        if len(self.rollout_rew) == 0:\n","            return (0.0, 0.0)\n","        R = 0.0\n","        returns = []\n","        for r, done in zip(reversed(self.rollout_rew), reversed(self.rollout_done)):\n","            R = r + gamma * R * (1.0 - done)\n","            returns.insert(0, R)\n","        returns = torch.tensor(returns, dtype=torch.float32).to(DEVICE)\n","\n","        obs = torch.tensor(np.array(self.rollout_obs)).float().to(DEVICE)\n","        acts = torch.tensor(self.rollout_act).long().to(DEVICE)\n","\n","        logits, values = self.policy(obs)\n","        logprobs = torch.log_softmax(logits, dim=-1)\n","        act_logp = logprobs[range(len(acts)), acts]\n","\n","        advantages = returns - values.detach()\n","        policy_loss = -(act_logp * advantages).mean()\n","        value_loss  = F.mse_loss(values, returns)\n","        entropy_loss = -(torch.softmax(logits, dim=-1) * logprobs).sum(-1).mean()\n","\n","        # Meta losses extra\n","        chaos_loss  = torch.tensor(random.random(), device=DEVICE)  # placeholder\n","        org_loss    = torch.tensor(random.random(), device=DEVICE)\n","        mem_loss    = torch.tensor(random.random(), device=DEVICE)\n","        stability   = torch.tensor(1.0 - min(1.0, (self.auto_manager.block_id + len(self.world_loss_hist))/50), device=DEVICE)\n","\n","        losses_vec = torch.stack([chaos_loss, org_loss, mem_loss,\n","                                  policy_loss.detach(), value_loss.detach(), stability])\n","        total_meta = self.meta_controller(losses_vec)\n","\n","        total_loss = policy_loss + 0.5 * value_loss - self.cfg[\"entropy_coef\"] * entropy_loss + total_meta\n","\n","        self.opt_policy.zero_grad()\n","        total_loss.backward()\n","        self.opt_policy.step()\n","\n","        # logs\n","        step = len(self.rollout_rew_total)\n","        self.writer.add_scalar(\"Loss/Policy\",    policy_loss.item(), step)\n","        self.writer.add_scalar(\"Loss/Value\",     value_loss.item(),  step)\n","        self.writer.add_scalar(\"Loss/Entropy\",   entropy_loss.item(),step)\n","        self.writer.add_scalar(\"Loss/TotalMeta\", total_meta.item(),  step)\n","\n","        return policy_loss.item(), value_loss.item()\n","\n","    # ------------------------------------------------------------\n","    # ROLLBACKS\n","    # ------------------------------------------------------------\n","    def _partial_rollback_if_needed(self, step_idx: int):\n","        if len(self.world_loss_hist) < self.world_loss_hist.maxlen:\n","            return\n","        avg_wl = np.mean(self.world_loss_hist)\n","        if avg_wl < self.cfg[\"rollback_world_loss_threshold\"]:\n","            return\n","        if step_idx - self.last_rollback_step < self.cfg[\"rollback_cooldown\"]:\n","            return\n","        # rollback parcial: reset world model\n","        print(f\"[Rollback] Partial rollback: resetting WorldModel (avg_wl={avg_wl:.3f})\")\n","        self.world_model.apply(self._init_weights)\n","        self.opt_world = optim.Adam(self.world_model.parameters(), lr=self.cfg[\"lr_world\"])\n","        self.world_loss_hist.clear()\n","        self.last_rollback_step = step_idx\n","\n","    @staticmethod\n","    def _init_weights(m):\n","        if isinstance(m, nn.Linear):\n","            nn.init.kaiming_normal_(m.weight)\n","            if m.bias is not None:\n","                nn.init.zeros_(m.bias)\n","\n","    # ------------------------------------------------------------\n","    # LOOP PRINCIPAL\n","    # ------------------------------------------------------------\n","    def run(self, total_steps: int = 10_000):\n","        global_step = 0\n","        episode_reward = 0.0\n","        self.rollout_rew_total: list[float] = []\n","        obs = self.state.copy()\n","\n","        while global_step < total_steps:\n","            obs_t = torch.tensor(obs, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n","            logits, _ = self.policy(obs_t)\n","            action = self.map_action_to_env(logits[0])\n","            next_obs, reward, done, trunc, info = self.env.step(action)\n","\n","            # store\n","            self.rollout_obs.append(obs)\n","            self.rollout_act.append(action)\n","            self.rollout_rew.append(reward)\n","            self.rollout_next.append(next_obs)\n","            self.rollout_done.append(done)\n","\n","            episode_reward += reward\n","            obs = next_obs.copy()\n","            global_step += 1\n","\n","            # update world model every n steps\n","            if global_step % 20 == 0:\n","                wl = self.update_world_model()\n","                self.writer.add_scalar(\"Loss/World\", wl, global_step)\n","\n","            # update policy\n","            if done or trunc or len(self.rollout_rew) >= 32:\n","                pl, vl = self.update_policy()\n","                self.rollout_obs.clear()\n","                self.rollout_act.clear()\n","                self.rollout_rew.clear()\n","                self.rollout_next.clear()\n","                self.rollout_done.clear()\n","                self.rollout_rew_total.append(episode_reward)\n","                self.writer.add_scalar(\"Return/Episode\", episode_reward, global_step)\n","\n","                if episode_reward > self.best_reward:\n","                    self.best_reward = episode_reward\n","\n","                # GenePool\n","                if global_step % self.cfg[\"gene_pool_interval\"] == 0:\n","                    self.gene_pool.add_gene(self.policy, self.meta_controller)\n","\n","                # troca de ambiente\n","                self.env_idx = (self.env_idx + 1) % len(self.envs)\n","                self.env = self.envs[self.env_idx]\n","                obs, _ = self.env.reset(seed=random.randint(0, 1_000_000))\n","                episode_reward = 0.0\n","            # auto expand/prune\n","            if global_step % 50 == 0:\n","                self.auto_manager.maybe_expand_or_prune(global_step, pl if isinstance(pl, float) else pl[0])\n","\n","            # partial rollback\n","            self._partial_rollback_if_needed(global_step)\n","\n","    # ------------------------------------------------------------\n","    # INTERFACE COM O CHAT\n","    # ------------------------------------------------------------\n","    def step_user_input(self, user_input: str) -> str:\n","        \"\"\"\n","        Gera um rascunho textual do \"pensamento\" atual:\n","        inclui estado, última recompensa média, etc.\n","        Esse draft será refinado pelo LLM (DeepSeek).\n","        \"\"\"\n","        draft = {\n","            \"current_env\": self.cfg[\"env_list\"][self.env_idx],\n","            \"state_snapshot\": self.state.tolist() if isinstance(self.state, np.ndarray) else str(self.state),\n","            \"last_reward\": self.rollout_rew[-1] if self.rollout_rew else 0.0,\n","            \"user_input\": user_input.strip(),\n","            \"policy_extra_blocks\": list(self.policy.extra_blocks.keys()),\n","            \"meta_weights\": self.meta_controller.weights.detach().cpu().tolist(),\n","        }\n","        return json.dumps(draft, ensure_ascii=False, indent=2)"]},{"cell_type":"code","source":["# lhra_core/kernel_manager.py\n","# ==============================================================\n","# Pequeno barramento de mensagens + scheduler de tarefas\n","# ==============================================================\n","\n","import time\n","import heapq\n","import typing as tp\n","from collections import deque, defaultdict\n","\n","Msg = dict  # alias semântico\n","\n","class Task:\n","    \"\"\"\n","    Tarefa genérica executada por algum módulo.\n","\n","    priority: float (0‑1; maior = executa antes)\n","    interval: steps entre execuções (>=1).\n","    condition: callable(state_dict) -> bool  (executa se True)\n","    callback:  função a ser chamada\n","    \"\"\"\n","    __slots__ = (\"name\", \"priority\", \"interval\",\n","                 \"condition\", \"callback\", \"last_exec\")\n","\n","    def __init__(\n","        self,\n","        name: str,\n","        callback: tp.Callable[[tp.Any], None],\n","        priority: float = 0.5,\n","        interval: int = 1,\n","        condition: tp.Callable[[dict], bool] | None = None,\n","    ):\n","        self.name = name\n","        self.priority = priority\n","        self.interval = max(1, interval)\n","        self.condition = condition or (lambda _: True)\n","        self.callback = callback\n","        self.last_exec = -self.interval\n","\n","    # -------- prioridade p/ heap ----------\n","    def __lt__(self, other: \"Task\"):\n","        return self.priority > other.priority\n","\n","\n","class KernelManager:\n","    \"\"\"\n","    Barramento super‑simples: módulos publicam métricas\n","    em self.state; Kernel decide quais Tasks disparar.\n","    \"\"\"\n","\n","    def __init__(self):\n","        self.state: dict = defaultdict(lambda: 0.0)\n","        self.tasks: dict[str, Task] = {}\n","        self._task_heap: list[Task] = []\n","\n","        # métricas históricas para consulta rápida\n","        self._history: dict[str, deque] = defaultdict(lambda: deque(maxlen=100))\n","\n","        # step contador\n","        self.step_idx: int = 0\n","\n","    # ------------------- STATE ------------------------\n","    def publish(self, key: str, value: float):\n","        self.state[key] = value\n","        self._history[key].append(value)\n","\n","    def get_state(self, key: str, default=0.0):\n","        return self.state.get(key, default)\n","\n","    # ------------------- TASKS ------------------------\n","    def register_task(\n","        self,\n","        name: str,\n","        callback: tp.Callable[[dict], None],\n","        *,\n","        priority: float = 0.5,\n","        interval: int = 1,\n","        condition: tp.Callable[[dict], bool] | None = None,\n","    ):\n","        task = Task(name, callback, priority, interval, condition)\n","        self.tasks[name] = task\n","        heapq.heappush(self._task_heap, task)\n","\n","    def set_priority(self, name: str, priority: float):\n","        if name in self.tasks:\n","            self.tasks[name].priority = priority\n","            # rebuild heap for simplicity\n","            self._task_heap = list(self.tasks.values())\n","            heapq.heapify(self._task_heap)\n","\n","    # ------------------- LOOP -------------------------\n","    def run_cycle(self):\n","        \"\"\"\n","        Executa tarefas cuja condição seja True\n","        e cujo intervalo tenha decorrido.\n","        \"\"\"\n","        executed = []\n","        while self._task_heap:\n","            task = heapq.heappop(self._task_heap)\n","            if self.step_idx - task.last_exec < task.interval:\n","                # ainda em cooldown; devolve\n","                executed.append(task)\n","                continue\n","            if not task.condition(self.state):\n","                executed.append(task)\n","                continue\n","            # Executa\n","            task.callback(self.state)\n","            task.last_exec = self.step_idx\n","            executed.append(task)\n","        # repõe heap\n","        for t in executed:\n","            heapq.heappush(self._task_heap, t)\n","        self.step_idx += 1\n","\n","    # ------------------- HISTÓRICO --------------------\n","    def mean(self, key: str) -> float:\n","        h = self._history[key]\n","        return sum(h) / len(h) if h else 0.0\n","\n","    def reset(self):\n","        self.state.clear()\n","        for dq in self._history.values():\n","            dq.clear()\n","        self.step_idx = 0"],"metadata":{"id":"FAoZKhX4lJQG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# lhra_core/focus_manager.py\n","# ==============================================================\n","# Define prioridades dinâmicas de cada módulo cognitivo\n","# ==============================================================\n","\n","import random\n","import typing as tp\n","from collections import deque\n","\n","class FocusManager:\n","    \"\"\"\n","    Recebe sinais de urgência e importância de cada módulo\n","    e devolve dicionário {module_name: prioridade 0‑1}.\n","    Implementa esquemas de:\n","        • little‑used boost\n","        • estabilidade vs. exploração\n","    \"\"\"\n","\n","    def __init__(self):\n","        # histórico de execuções p/ fairness\n","        self._last_run: dict[str, int] = defaultdict(int)\n","        self.step_idx: int = 0\n","\n","    def decide_focus(\n","        self,\n","        signals: dict[str, dict[str, float]],\n","        base_priorities: dict[str, float] | None = None,\n","    ) -> dict[str, float]:\n","        \"\"\"\n","        signals[module] = {\"urgency\":..,\"importance\":..}\n","        Retorna dict de prioridades normalizadas.\n","        \"\"\"\n","        base_priorities = base_priorities or {}\n","        raw: dict[str, float] = {}\n","\n","        for mod, sig in signals.items():\n","            urg = sig.get(\"urgency\", 0.5)\n","            imp = sig.get(\"importance\", 0.5)\n","            base = base_priorities.get(mod, 0.5)\n","            # distância temporal: módulos que não rodam há muito\n","            lag = self.step_idx - self._last_run.get(mod, 0)\n","            lag_boost = min(0.3, lag / 1000.0)\n","            score = 0.5 * urg + 0.4 * imp + 0.1 * base + lag_boost\n","            raw[mod] = score\n","\n","        # normaliza\n","        total = sum(raw.values()) or 1e-6\n","        norm = {m: v / total for m, v in raw.items()}\n","\n","        # registra quem vai rodar\n","        chosen = {m: p for m, p in norm.items() if p > 0.15}  # cutoff\n","        for m in chosen:\n","            self._last_run[m] = self.step_idx\n","\n","        self.step_idx += 1\n","        return chosen"],"metadata":{"id":"NTFsikaclc4E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# lhra_core/world_model.py\n","# ==========================================================\n","# Pequeno World‑Model (dinâmica + recompensa) em PyTorch\n","# ==========================================================\n","import torch, torch.nn as nn, torch.nn.functional as F\n","from torch.optim import Adam\n","from collections import deque\n","import random, typing as tp\n","\n","class WorldModel(nn.Module):\n","    \"\"\"\n","    Modela P(s_{t+1}|s_t,a_t)  e  R(s_t,a_t)\n","    • state_dim  : dimensão da observação (float list)\n","    • action_dim : dimensão/num de ações (discrete -> int)\n","    \"\"\"\n","    def __init__(self, state_dim:int, action_dim:int, lr:float=3e‑4,\n","                 buffer_size:int=50_000, device:str|None=None):\n","        super().__init__()\n","        self.state_dim  = state_dim\n","        self.action_dim = action_dim\n","        self.device     = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","        hid = 128\n","        self.dyn = nn.Sequential(\n","            nn.Linear(state_dim+action_dim, hid),\n","            nn.ReLU(),\n","            nn.Linear(hid, hid),\n","            nn.ReLU(),\n","            nn.Linear(hid, state_dim)          # Δs  (modela diferença)\n","        )\n","        self.rew = nn.Sequential(\n","            nn.Linear(state_dim+action_dim, hid),\n","            nn.Tanh(),\n","            nn.Linear(hid, 1)\n","        )\n","        self.to(self.device)\n","        self.opt  = Adam(self.parameters(), lr=lr)\n","        self.buf  = deque(maxlen=buffer_size)\n","        self.steps= 0\n","\n","    # ---------- Buffer / coleta ----------\n","    def store(self, s:list[float], a:int, r:float, s2:list[float]):\n","        self.buf.append((torch.tensor(s).float(),\n","                         torch.tensor([a]).long(),\n","                         torch.tensor([r]).float(),\n","                         torch.tensor(s2).float()))\n","\n","    # ---------- Treinamento incremental ----------\n","    def update(self, batch:int=64, iters:int=5)->float:\n","        if len(self.buf) < batch: return 0.0\n","        losses=[]\n","        for _ in range(iters):\n","            batch_data = random.sample(self.buf, batch)\n","            s,a,r,s2 = map(lambda x: torch.stack(x).to(self.device),\n","                           zip(*batch_data))\n","            a_onehot = F.one_hot(a.squeeze(), num_classes=self.action_dim).float()\n","            sa       = torch.cat([s, a_onehot], dim=-1)\n","\n","            pred_deltas = self.dyn(sa)\n","            pred_s2     = s + pred_deltas\n","            dyn_loss    = F.mse_loss(pred_s2, s2)\n","\n","            pred_r      = self.rew(sa).squeeze()\n","            rew_loss    = F.mse_loss(pred_r, r.squeeze())\n","\n","            loss = dyn_loss + 0.5*rew_loss\n","            self.opt.zero_grad(); loss.backward(); self.opt.step()\n","            losses.append(loss.item())\n","            self.steps += 1\n","        return sum(losses)/len(losses)\n","\n","    # ---------- Inference ----------\n","    @torch.no_grad()\n","    def predict(self, s:list[float], a:int)->tuple[list[float], float]:\n","        s_t = torch.tensor(s, device=self.device).float().unsqueeze(0)\n","        a_oh= F.one_hot(torch.tensor([a]), num_classes=self.action_dim).float().to(self.device)\n","        sa  = torch.cat([s_t, a_oh], -1)\n","        s2  = s_t + self.dyn(sa)\n","        r   = self.rew(sa).squeeze()\n","        return s2.cpu().view(-1).tolist(), float(r.cpu())\n","\n","# ==========================================================\n","# Interface utilitária para LHRA_Agent\n","# ==========================================================\n","def init_world_model(cfg:dict)->WorldModel:\n","    return WorldModel(cfg[\"state_dim\"], cfg[\"action_dim\"], lr=cfg.get(\"wm_lr\",3e-4))"],"metadata":{"id":"wyBQxtlAleUk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# lhra_core/chaos_generator.py\n","# ==========================================================\n","# Gera ruído fractal / browniano para “forças de caos” internas\n","# ==========================================================\n","import numpy as np, math, random\n","\n","class ChaosGenerator:\n","    \"\"\"Gera sequências de ruído coerente (Perlin 1D simplificado).\"\"\"\n","    def __init__(self, seed:int|None=None, scale:float=.3):\n","        self.scale  = scale\n","        random.seed(seed or 0xA1B2C3)\n","\n","        # Gradientes em pontos inteiros\n","        self.grad = {i:random.uniform(-1,1) for i in range(10_000)}\n","\n","    def _grad(self, i:int)->float:\n","        if i not in self.grad:\n","            self.grad[i] = random.uniform(-1,1)\n","        return self.grad[i]\n","\n","    def _fade(self, t:float)->float:\n","        # curva de suavização 6t^5-15t^4+10t^3\n","        return t*t*t*(t*(t*6-15)+10)\n","\n","    def noise(self, x:float)->float:\n","        x0 = math.floor(x); x1 = x0+1\n","        sx = self._fade(x - x0)\n","        n0 = self._grad(x0)*(x-x0)\n","        n1 = self._grad(x1)*(x-x1)\n","        return (1-sx)*n0 + sx*n1\n","\n","    # Sequência temporal de ruído\n","    def sequence(self, t0:float, steps:int, step_size:float=.01)->list[float]:\n","        return [self.noise(t0+i*step_size)*self.scale for i in range(steps)]"],"metadata":{"id":"D_x6eu-Byze8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# lhra_core/fractal_memory.py\n","# ==========================================================\n","# Estrutura de “memória fractal” simples (árvore HAMT‑like)\n","# ==========================================================\n","import hashlib, json, os\n","from collections import defaultdict\n","from pathlib import Path\n","import typing as tp\n","\n","def _hash(key:str)->int:\n","    return int(hashlib.sha256(key.encode()).hexdigest(),16)\n","\n","class Node:\n","    __slots__=(\"children\",\"value\")\n","    def __init__(self):\n","        self.children: dict[int,\"Node\"] = {}\n","        self.value: tp.Any = None\n","\n","class FractalMemory:\n","    \"\"\"\n","    Armazena pares (chave -> valor) numa trie de hashing.\n","    O “profundidade” é limitado para não explodir RAM.\n","    \"\"\"\n","    def __init__(self, depth:int=5, fanout:int=16,\n","                 file:Path|str=\"fractal_mem.json\"):\n","        self.root   = Node()\n","        self.depth  = depth\n","        self.fanout = fanout\n","        self.file   = Path(file)\n","        if self.file.exists(): self._load()\n","\n","    # ------------ helpers ---------------\n","    def _index_seq(self, key:str)->list[int]:\n","        h=_hash(key)\n","        seq=[]\n","        for _ in range(self.depth):\n","            seq.append(h % self.fanout)\n","            h//= self.fanout\n","        return seq\n","\n","    # ------------ operations ------------\n","    def put(self, key:str, value:tp.Any):\n","        node=self.root\n","        for idx in self._index_seq(key):\n","            if idx not in node.children: node.children[idx]=Node()\n","            node=node.children[idx]\n","        node.value=value\n","        self._dump()\n","\n","    def get(self, key:str, default=None):\n","        node=self.root\n","        for idx in self._index_seq(key):\n","            if idx not in node.children: return default\n","            node=node.children[idx]\n","        return node.value if node.value is not None else default\n","\n","    def _dump(self):\n","        # serialização bem simples (profunda)\n","        out={}\n","        def rec(n:Node,pfx:str):\n","            if n.value is not None: out[pfx] = n.value\n","            for k,ch in n.children.items():\n","                rec(ch, pfx+hex(k)[2:])\n","        rec(self.root,\"\")\n","        self.file.write_text(json.dumps(out))\n","\n","    def _load(self):\n","        data=json.loads(self.file.read_text())\n","        for k,v in data.items(): self.put(k,v)\n","\n","# ==========================================================\n","# Interface para outros módulos\n","# ==========================================================\n","def init_fractal_memory(cfg:dict)->FractalMemory:\n","    return FractalMemory(depth=cfg.get(\"fm_depth\",5),\n","                         fanout=cfg.get(\"fm_fanout\",16),\n","                         file=cfg.get(\"fm_file\",\"fractal_mem.json\"))"],"metadata":{"id":"ElyLF9VEy782"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# lhra_core/semantic_memory.py\n","# ==========================================================\n","import json, os, threading, typing as tp\n","from pathlib import Path\n","\n","class SemanticMemory:\n","    \"\"\"\n","    Memória factual de longo prazo salva em disco.\n","    • key -> value  (qualquer JSON‑serializável)\n","    • thread‑safe usando Lock simples\n","    \"\"\"\n","    def __init__(self, file: str | Path = \"semantic_memory.json\"):\n","        self.path   = Path(file)\n","        self.data: dict[str, tp.Any] = {}\n","        self._lock  = threading.Lock()\n","        self._load()\n","\n","    # -------- Básico ----------\n","    def store(self, key: str, value: tp.Any):\n","        with self._lock:\n","            self.data[key] = value\n","            self._save()\n","\n","    def retrieve(self, key: str, default=None):\n","        return self.data.get(key, default)\n","\n","    def search_prefix(self, prefix:str)->dict[str,tp.Any]:\n","        return {k:v for k,v in self.data.items() if k.startswith(prefix)}\n","\n","    # -------- Persistência ----------\n","    def _save(self):\n","        self.path.write_text(json.dumps(self.data, indent=2))\n","\n","    def _load(self):\n","        if self.path.exists():\n","            try:\n","                self.data = json.loads(self.path.read_text())\n","            except Exception:\n","                self.data = {}\n","\n","# helper\n","def init_semantic_memory(cfg:dict)->SemanticMemory:\n","    return SemanticMemory(cfg.get(\"semantic_file\", \"semantic_memory.json\"))"],"metadata":{"id":"rpUObjFlzAa6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# lhra_core/symbolic_reasoner.py\n","# ==========================================================\n","import numpy as np\n","from sklearn.cluster import KMeans\n","from collections import defaultdict\n","from .semantic_memory import SemanticMemory\n","\n","try:\n","    import networkx as nx                  # opcional (visualização de regras)\n","except ImportError:\n","    nx = None\n","\n","class SymbolicReasoner:\n","    \"\"\"\n","    Extrai padrões + gera regras simbólicas sobre (estado, ação, recompensa).\n","    Também cria 'sub‑goals' para GoalManager baseado nos clusters.\n","    \"\"\"\n","    def __init__(self, state_dim:int, semantic_mem:SemanticMemory,\n","                 n_clusters:int=6, random_state:int=42):\n","        self.state_dim     = state_dim\n","        self.semantic_mem  = semantic_mem\n","        self.n_clusters    = n_clusters\n","        self.kmeans        = KMeans(n_clusters=n_clusters,\n","                                    random_state=random_state)\n","        self._buffer: list[np.ndarray] = []   # últimos estados coletados\n","        self.cluster_centroids: np.ndarray|None = None\n","        self.rules: dict[int,list[str]] = defaultdict(list)\n","\n","    # ---------- coleta ----------\n","    def observe_state(self, state:list[float]):\n","        self._buffer.append(np.array(state))\n","        if len(self._buffer) > 5000:          # mantém ~5k\n","            self._buffer = self._buffer[-5000:]\n","\n","    # ---------- processamento ----------\n","    def extract_rules(self)->None:\n","        if len(self._buffer) < self.n_clusters*5:   # precisa de dados\n","            return\n","        X = np.stack(self._buffer)\n","        self.kmeans.fit(X)\n","        self.cluster_centroids = self.kmeans.cluster_centers_\n","\n","        # Exemplo simples: regra = média > mediana em certa dimensão\n","        dims = min(4, self.state_dim)        # examina 4 primeiras dims\n","        self.rules.clear()\n","        for i,c in enumerate(self.cluster_centroids):\n","            cluster_name = f\"cluster_{i}\"\n","            generated=[]\n","            for d in range(dims):\n","                generated.append(\n","                    f\"IF state[{d}] ≈ {c[d]:.2f} THEN likely in {cluster_name}\"\n","                )\n","            self.rules[i] = generated\n","            # salva no SemanticMemory\n","            self.semantic_mem.store(f\"centroid:{cluster_name}\", c.tolist())\n","\n","    # ---------- sub‑goals ----------\n","    def generate_sub_goals(self)->list[dict]:\n","        goals=[]\n","        if self.cluster_centroids is None: return goals\n","        for idx,c in enumerate(self.cluster_centroids):\n","            goals.append({\n","                \"type\": \"explore_cluster\",\n","                \"target\": c.tolist(),\n","                \"id\": idx\n","            })\n","        return goals\n","\n","    # ---------- visualização opcional ----------\n","    def export_graph(self, file=\"rules_graph.png\"):\n","        if nx is None or not self.rules: return\n","        G = nx.DiGraph()\n","        for cid, rs in self.rules.items():\n","            for r in rs: G.add_edge(f\"cluster_{cid}\", r)\n","        nx.drawing.nx_pydot.write_dot(G, \"rules.dot\")\n","        os.system(\"dot -Tpng rules.dot -o \"+file)"],"metadata":{"id":"SpS-X3yxzBsi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# lhra_core/planning_module.py\n","# ==========================================================\n","import time, typing as tp\n","from collections import deque\n","\n","class Task:\n","    def __init__(self, name:str, params:dict|None=None, cond:tp.Callable[...,bool]|None=None):\n","        self.name   = name\n","        self.params = params or {}\n","        self.cond   = cond or (lambda *_: True)\n","        self.done   = False\n","\n","    def __repr__(self): return f\"Task({self.name})\"\n","\n","class PlanningModule:\n","    \"\"\"\n","    HTN simplificado:  goal -> lista hierárquica de Sub‑Tarefas (tasks_queue)\n","    \"\"\"\n","    def __init__(self):\n","        self.current_plan: deque[Task] = deque()\n","        self.cot: list[str] = []             # Chain‑of‑Thought textual\n","\n","    def build_hierarchical_plan(self, goal:dict)->None:\n","        self.current_plan.clear(); self.cot.clear()\n","        gtype = goal.get(\"type\")\n","\n","        if gtype == \"explore_cluster\":\n","            self.cot.append(f\"— Meta: Explorar {goal['id']}\")\n","            self.current_plan.append(Task(\"NavigateToCentroid\",\n","                                   {\"centroid\":goal[\"target\"]}))\n","            self.current_plan.append(Task(\"CollectObservations\",\n","                                   {\"duration\": 50}))\n","        else:\n","            self.cot.append(f\"— Meta genérica {gtype}\")\n","            self.current_plan.append(Task(\"Idle\", {\"duration\":20}))\n","\n","    # Chamado a cada step\n","    def step(self, env_state)->list[str]:\n","        if not self.current_plan: return []\n","\n","        task=self.current_plan[0]\n","        if task.cond(env_state):       # pronto para executar\n","            self.execute(task, env_state)\n","            task.done=True\n","            self.current_plan.popleft()\n","        return self.cot[-1:]           # devolve última CoT para log\n","\n","    def execute(self, task:Task, env_state):\n","        self.cot.append(\n","            f\"{time.time():.0f}: Executando {task.name} com {task.params}\"\n","        )\n","        # efeito fictício; integração real depende da politica"],"metadata":{"id":"iJRZlZxlzFBX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# lhra_core/goal_manager.py\n","# ==========================================================\n","import time, uuid, typing as tp\n","\n","class Goal:\n","    def __init__(self, goal_dict:dict):\n","        self.id        = goal_dict.get(\"uid\", str(uuid.uuid4()))\n","        self.type      = goal_dict[\"type\"]\n","        self.payload   = goal_dict.get(\"target\", None)\n","        self.deadline  = goal_dict.get(\"deadline\", time.time()+600)\n","        self.progress  = 0.0\n","        self.done      = False\n","\n","class GoalManager:\n","    \"\"\"\n","    Gerencia fila de metas: adicionar, atualizar progresso, remover.\n","    \"\"\"\n","    def __init__(self):\n","        self.active: dict[str, Goal] = {}\n","\n","    def add_goals(self, goals:list[dict]):\n","        for g in goals:\n","            goal = Goal(g)\n","            self.active[goal.id] = goal\n","\n","    def update_progress(self, env_state):\n","        # exemplo: marcar done se perto do payload/centroide\n","        for goal in self.active.values():\n","            if goal.type==\"explore_cluster\":\n","                dist = float(sum((a-b)**2 for a,b in zip(env_state,goal.payload))**0.5)\n","                goal.progress = max(goal.progress, 1/(dist+1e‑4))\n","                if dist < .05: goal.done=True\n","\n","    def remove_done(self):\n","        self.active = {k:v for k,v in self.active.items() if not v.done}\n","\n","    # Acesso externo\n","    def primary_goal(self)->Goal|None:\n","        if not self.active: return None\n","        # prioriza a maior urgência (deadline próxima)\n","        return min(self.active.values(), key=lambda g:g.deadline)"],"metadata":{"id":"Wt0nXFCyzJgz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# lhra_core/auto_expand_prune.py\n","# ==========================================================\n","\"\"\"\n","Gerencia expansão (adiciona blocos) e poda (remove blocos pouco relevantes)\n","da política `torch.nn.Module`.  Mantém até K checkpoints em heap ordenado\n","pelo melhor `avg_reward`.\n","\"\"\"\n","from __future__ import annotations\n","import torch, heapq, time, copy\n","from collections import deque\n","from dataclasses import dataclass, field\n","\n","@dataclass(order=True)\n","class _Chkpt:\n","    score: float\n","    ts   : float = field(compare=False)\n","    data : dict  = field(compare=False)\n","\n","class AutoExpandPruneManager:\n","    def __init__(self,\n","                 policy: torch.nn.Module,\n","                 optimizer: torch.optim.Optimizer,\n","                 max_extras:int = 4,\n","                 expand_threshold: float = 0.05,      # perda ↑ 5 %\n","                 prune_threshold : float =-0.05,      # perda ↓ 5 %\n","                 cooldown_steps  : int   = 500,\n","                 top_k_ckpt      : int   = 3):\n","        self.policy  = policy\n","        self.optimizer = optimizer\n","        self.max_extras = max_extras\n","        self.expand_threshold = expand_threshold\n","        self.prune_threshold  = prune_threshold\n","        self.cooldown_steps   = cooldown_steps\n","        self.last_action_step = 0\n","        self.step_counter     = 0\n","\n","        self.block_stats: dict[str,float] = {}\n","        self.checkpoints : list[_Chkpt] = []\n","        self.top_k_ckpt  = top_k_ckpt\n","\n","    # -------------------------------------------------------\n","    # checkpoint helpers\n","    def _make_full_state(self)->dict:\n","        return {\n","            \"policy\"   : copy.deepcopy(self.policy.state_dict()),\n","            \"optim\"    : copy.deepcopy(self.optimizer.state_dict()),\n","        }\n","    def push_ckpt(self, score:float):\n","        heapq.heappush(self.checkpoints,\n","                       _Chkpt(score, time.time(), self._make_full_state()))\n","        # mantêm heap pequeno\n","        if len(self.checkpoints) > self.top_k_ckpt:\n","            heapq.heappop(self.checkpoints)\n","\n","    def restore_best(self):\n","        if not self.checkpoints: return False\n","        best = max(self.checkpoints).data\n","        self.policy.load_state_dict(best[\"policy\"])\n","        self.optimizer.load_state_dict(best[\"optim\"])\n","        return True\n","\n","    # -------------------------------------------------------\n","    # expansão / poda\n","    def maybe_expand_or_prune(self,\n","                              current_loss: float,\n","                              ref_loss    : float):\n","        self.step_counter += 1\n","        if self.step_counter - self.last_action_step < self.cooldown_steps:\n","            return                                  # em cooldown\n","\n","        delta = (current_loss - ref_loss) / (abs(ref_loss)+1e-8)\n","\n","        if delta > self.expand_threshold and self._n_extras() < self.max_extras:\n","            self._expand_block()\n","        elif delta < self.prune_threshold and self._n_extras()>0:\n","            self._prune_block()\n","        # else: nada\n","\n","    # ---------- implementação ----------\n","    def _n_extras(self)->int:\n","        return sum(1 for n,_ in self.policy.named_modules() if n.startswith(\"extra_\"))\n","    def _expand_block(self):\n","        idx = self._n_extras()\n","        new_block = torch.nn.Sequential(\n","            torch.nn.Linear(128, 128),\n","            torch.nn.BatchNorm1d(128),\n","            torch.nn.ReLU(),\n","        )\n","        block_name = f\"extra_{idx}\"\n","        setattr(self.policy, block_name, new_block)\n","        self.block_stats[block_name] = 1.0          # peso inicial\n","        self.last_action_step = self.step_counter\n","        print(f\"[AutoExpand] adicionou {block_name}\")\n","\n","    def _prune_block(self):\n","        # Avalia importância via grad + peso\n","        self._update_block_stats()\n","        # menor importância\n","        block_name = min(self.block_stats, key=self.block_stats.get)\n","        delattr(self.policy, block_name)\n","        self.block_stats.pop(block_name, None)\n","        self.last_action_step = self.step_counter\n","        print(f\"[AutoPrune] removeu {block_name}\")\n","\n","    # ---------- importância ----------\n","    def _update_block_stats(self):\n","        for name, module in self.policy.named_modules():\n","            if not name.startswith(\"extra_\"): continue\n","            with torch.no_grad():\n","                w_norm = sum(p.norm().item() for p in module.parameters())\n","                # gradiente médio\n","                g_norm = sum((p.grad.norm().item() if p.grad is not None else 0)\n","                             for p in module.parameters())\n","            self.block_stats[name] = 0.7*w_norm + 0.3*g_norm"],"metadata":{"id":"snA_mwyOzOcY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# lhra_core/gene_pool.py\n","# ==========================================================\n","\"\"\"\n","Mecanismo evolucionário simples:\n","• Armazena top‑N 'genes' (snapshot de pesos + meta‑params).\n","• Mutação leve (+/‑5 %) nos pesos do meta‑controlador (DynamicWeightAdapter).\n","• Torneio offline de 600 steps com seeds fixas.\n","\"\"\"\n","from __future__ import annotations\n","import copy, random, torch, numpy as np\n","\n","class Gene:\n","    def __init__(self, state_dict:dict, meta_weights:torch.Tensor,\n","                 score:float, seed:int):\n","        self.state_dict   = state_dict\n","        self.meta_weights = meta_weights.clone()\n","        self.score        = score\n","        self.seed         = seed\n","\n","class GenePool:\n","    def __init__(self, capacity:int=5):\n","        self.capacity = capacity\n","        self.pool: list[Gene] = []\n","\n","    # ---------- armazenamento ----------\n","    def consider(self, gene:Gene):\n","        self.pool.append(gene)\n","        self.pool.sort(key=lambda g: g.score, reverse=True)\n","        self.pool = self.pool[:self.capacity]\n","\n","    # ---------- mutação ----------\n","    @staticmethod\n","    def mutate_meta(meta:torch.Tensor, noise:float=0.05)->torch.Tensor:\n","        with torch.no_grad():\n","            return meta * (1.0 + noise*torch.randn_like(meta))\n","\n","    # ---------- seleção ----------\n","    def sample_best(self)->Gene|None:\n","        return self.pool[0] if self.pool else None\n","\n","    # ---------- torneio offline ----------\n","    def tournament(self, agent_factory, env_factory,\n","                   steps:int=600, seed:int=123)->None:\n","        if not self.pool: return\n","        challenger = random.choice(self.pool)\n","        mutated_meta = self.mutate_meta(challenger.meta_weights)\n","        agent = agent_factory(challenger.state_dict, mutated_meta)\n","\n","        env = env_factory(seed)\n","        total=0\n","        state,_ = env.reset(seed=seed)\n","        for _ in range(steps):\n","            action = agent.act(state)\n","            state, rew, term, trunc,_ = env.step(action)\n","            total += rew\n","            if term or trunc: state,_ = env.reset()\n","        # Armazena gene se melhor\n","        self.consider(Gene(agent.state_dict(), mutated_meta, total/steps, seed))"],"metadata":{"id":"ss5d0DU5zWe5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# lhra_core/code_synthesizer.py\n","# ==========================================================\n","\"\"\"\n","Gera código Python dinamicamente, carrega como módulo e injeta bloco\n","na política. Remove se não houver ganho >5 % após `eval_window` steps.\n","\"\"\"\n","import importlib.util, inspect, os, random, time, types, uuid, textwrap\n","from pathlib import Path\n","import torch, shutil\n","\n","PLUGIN_DIR = Path(\"generated_plugins\")\n","PLUGIN_DIR.mkdir(exist_ok=True)\n","\n","class ActivePlugin:\n","    def __init__(self, name:str, inj_step:int, base_score:float):\n","        self.name       = name\n","        self.inj_step   = inj_step\n","        self.base_score = base_score\n","        self.removed    = False\n","\n","class CodeSynthesizer:\n","    def __init__(self, policy:torch.nn.Module,\n","                 eval_window:int=300, min_improve:float=0.05):\n","        self.policy = policy\n","        self.eval_window = eval_window\n","        self.min_improve = min_improve\n","        self.active_plugins: dict[str, ActivePlugin] = {}\n","\n","    # ---------- ciclo principal ----------\n","    def step(self, step_idx:int, current_score:float):\n","        # injeta plugin de tempos em tempos\n","        if step_idx % 800 == 0:\n","            self._inject_plugin(step_idx, current_score)\n","        # avalia plugins ativos\n","        for name, plug in list(self.active_plugins.items()):\n","            if plug.removed: continue\n","            if step_idx - plug.inj_step >= self.eval_window:\n","                delta = (current_score - plug.base_score)/abs(plug.base_score+1e-9)\n","                if delta < self.min_improve:\n","                    self._remove_plugin(name)\n","                else:\n","                    # mantêm mas atualiza base_score\n","                    plug.base_score = current_score\n","                    plug.inj_step   = step_idx\n","\n","    # ---------- geração ----------\n","    def _inject_plugin(self, step_idx:int, base_score:float):\n","        mode = random.choice([\"linear\",\"residual\",\"transformer\"])\n","        code_str, class_name = self._generate_code(mode)\n","        file_path = PLUGIN_DIR/f\"plug_{uuid.uuid4().hex}.py\"\n","        file_path.write_text(code_str)\n","\n","        spec = importlib.util.spec_from_file_location(file_path.stem, file_path)\n","        module = importlib.util.module_from_spec(spec); spec.loader.exec_module(module)\n","        block_cls = getattr(module, class_name)\n","        block = block_cls()\n","\n","        plug_name = f\"plug_{len(self.active_plugins)}\"\n","        setattr(self.policy, plug_name, block)\n","        self.active_plugins[plug_name] = ActivePlugin(plug_name, step_idx, base_score)\n","        print(f\"[CodeSynth] injetou {plug_name} ({mode})\")\n","\n","    # ---------- remoção ----------\n","    def _remove_plugin(self, plug_name:str):\n","        try:\n","            delattr(self.policy, plug_name)\n","            self.active_plugins[plug_name].removed=True\n","            print(f\"[CodeSynth] removeu {plug_name} (sem ganho)\")\n","        except AttributeError:\n","            pass\n","\n","    # ---------- templates ----------\n","    def _generate_code(self, mode:str)->tuple[str,str]:\n","        class_name = \"ExtraBlock\"\n","        if mode == \"linear\":\n","            body = \"\"\"\n","self.block = torch.nn.Linear(128, 128)\n","def forward(self, x):\n","    return torch.relu(self.block(x))\n","\"\"\"\n","        elif mode == \"residual\":\n","            body = \"\"\"\n","self.block = torch.nn.Sequential(\n","    torch.nn.Linear(128,128), torch.nn.ReLU(),\n","    torch.nn.Linear(128,128))\n","def forward(self, x):\n","    return x + 0.5*self.block(x)\n","\"\"\"\n","        elif mode == \"transformer\":\n","            # requer transformers; fallback para linear se indisponível\n","            try:\n","                import transformers\n","                body = \"\"\"\n","from transformers import DistilBertModel\n","self.bert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n","def forward(self, x):\n","    out = self.bert(inputs_embeds=x.unsqueeze(0)).last_hidden_state.squeeze(0)\n","    return out[:, :128]  # projeta\n","\"\"\"\n","            except ImportError:\n","                body = \"\"\"\n","self.block = torch.nn.Linear(128,128)\n","def forward(self,x): return torch.relu(self.block(x))\n","\"\"\"\n","        code = f'''\n","import torch\n","class {class_name}(torch.nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","{textwrap.indent(body, \"        \")}\n","'''\n","        return code, class_name"],"metadata":{"id":"NEBXP9z8zaOY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# lhra_core/metrics_coordinator.py\n","# ==========================================================\n","\"\"\"\n","Unifica métricas de caos, organização, memória, policy_loss, world_loss e estabilidade.\n","Mantém histórico (deque) para normalização por Z‑score ou min‑max.\n","\"\"\"\n","from __future__ import annotations\n","from collections import deque\n","import numpy as np\n","\n","_METRICS = (\"chaos\",\"org\",\"mem\",\"policy\",\"world\",\"stability\")\n","\n","class MetricsCoordinator:\n","    def __init__(self, hist:int=500):\n","        self.hist = hist\n","        self.buffers = {k: deque(maxlen=hist) for k in _METRICS}\n","        self.last_snapshot:dict[str,float] = {k:0. for k in _METRICS}\n","\n","    # -------------------------------------------------------\n","    def update(self, step:int,\n","               chaos:float, org:float, mem:float,\n","               p_loss:float, w_loss:float):\n","        stability = self._compute_stability(step)\n","        vals = dict(chaos=chaos, org=org, mem=mem,\n","                    policy=p_loss, world=w_loss,\n","                    stability=stability)\n","        for k,v in vals.items(): self.buffers[k].append(v)\n","        self.last_snapshot = self._normalize(vals)\n","\n","    # -------------------------------------------------------\n","    def snapshot(self)->dict[str,float]:\n","        return self.last_snapshot\n","\n","    # -------------------------------------------------------\n","    def _normalize(self, vals:dict[str,float])->dict[str,float]:\n","        out = {}\n","        for k,v in vals.items():\n","            buf = np.array(self.buffers[k]) if self.buffers[k] else np.array([v])\n","            mean, std = buf.mean(), buf.std()+1e-6\n","            out[k] = (v-mean)/std\n","        return out\n","\n","    # -------------------------------------------------------\n","    # simples: estabilidade = 1 - (# rollback + # expand) / janela\n","    def _compute_stability(self, step:int)->float:\n","        tot = sum(len(b) for b in self.buffers.values())\n","        if tot==0: return 1.\n","        changes = len(self.buffers[\"policy\"])   # proxy: mudanças em loss\n","        return max(0., 1. - changes/tot)"],"metadata":{"id":"GFXJ4Z6_zfnM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# lhra_core/cognitive_scheduler.py\n","# ==========================================================\n","\"\"\"\n","Atenção baseada em métricas: devolve um dicionário prioridade∈[0,1]\n","para cada módulo registrado.\n","\"\"\"\n","from __future__ import annotations\n","import numpy as np\n","\n","class CognitiveScheduler:\n","    def __init__(self, modules:list[str]):\n","        self.modules = modules\n","        # pesos fixos de influência das métricas -> prioridade\n","        self.weights = {\n","            \"world_model\" : np.array([+0.0,-0.2,-0.1,-0.2,+0.9,-0.3]),\n","            \"symbolic\"    : np.array([+0.2,+0.8,+0.2,-0.1,-0.1,-0.1]),\n","            \"memory\"      : np.array([+0.0,+0.1,+0.9,-0.2,-0.1,+0.0]),\n","            \"planner\"     : np.array([+0.1,+0.3,+0.1,+0.0,+0.0,-0.1]),\n","            \"gene_pool\"   : np.array([-0.1,-0.2,-0.2,+0.3,+0.2,-0.4]),\n","            \"synth\"       : np.array([-0.3,-0.3,-0.3,+0.8,+0.2,-0.2]),\n","        }\n","\n","    def compute_priorities(self, norm_metrics:dict[str,float])->dict[str,float]:\n","        vec = np.array([norm_metrics[k] for k in\n","                        (\"chaos\",\"org\",\"mem\",\"policy\",\"world\",\"stability\")])\n","        prios = {}\n","        for m in self.modules:\n","            w = self.weights.get(m, np.zeros_like(vec))\n","            score = float(np.tanh(np.dot(w,vec)))   # [-1,1]  → prioridade\n","            prios[m] = max(0., score)               # corta negativos\n","        # normaliza soma=1 (se todos zero, divide evita 0)\n","        s = sum(prios.values()) or 1.\n","        return {m:p/s for m,p in prios.items()}"],"metadata":{"id":"uyMreXiXzizT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# lhra_core/tool_manager.py\n","# ==========================================================\n","\"\"\"\n","Ferramentas externas registráveis. Uso:\n","    res = tool_manager.invoke(\"web_search\", query=\"LLMs\")\n","\"\"\"\n","import requests, json, os\n","from pathlib import Path\n","\n","class ToolManager:\n","    def __init__(self):\n","        self.tools:dict[str,callable] = {}\n","        # registra ferramentas padrão\n","        self.register(\"web_search\",   self._web_search)\n","        self.register(\"file_read\",    self._file_read)\n","\n","    def register(self, name:str, func:callable):\n","        self.tools[name]=func\n","\n","    def invoke(self, name:str, **kwargs):\n","        if name not in self.tools:\n","            raise ValueError(f\"Tool {name} não existe\")\n","        return self.tools[name](**kwargs)\n","\n","    # -------------------------------------------------------\n","    # Ferramentas padrão\n","    def _web_search(self, query:str, top_k:int=3):\n","        url=f\"https://duckduckgo.com/?q={query}&format=json\"\n","        try:\n","            data=requests.get(url, timeout=10).json()\n","            res=[a[\"Text\"] for a in data.get(\"RelatedTopics\",[])][:top_k]\n","            return res or [\"Sem resultados.\"]\n","        except Exception as e:\n","            return [f\"Erro: {e}\"]\n","\n","    def _file_read(self, path:str, max_chars:int=2000):\n","        p = Path(path)\n","        if not p.exists(): return f\"Arquivo {path} não encontrado.\"\n","        return p.read_text()[:max_chars]"],"metadata":{"id":"QV64Wj6azmEn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# lhra_core/narrative_module.py\n","# ==========================================================\n","\"\"\"\n","Armazena raciocínio textual (“chain‑of‑thought”) e permite gerar narrativa\n","contrafactual simples para debugging.\n","\"\"\"\n","from __future__ import annotations\n","from pathlib import Path, PurePath\n","import json, time\n","\n","class NarrativeModule:\n","    def __init__(self, log_dir=\"narratives\"):\n","        self.dir = Path(log_dir); self.dir.mkdir(exist_ok=True)\n","        self.buffer = []\n","\n","    def log_step(self, state, action, reward, info:str=\"\"):\n","        entry = dict(t=time.time(),\n","                     s=str(state)[:120],\n","                     a=str(action),\n","                     r=float(reward),\n","                     info=info)\n","        self.buffer.append(entry)\n","\n","    def flush(self, episode:int):\n","        if not self.buffer: return\n","        f = self.dir/f\"ep_{episode}.json\"\n","        f.write_text(json.dumps(self.buffer, indent=2))\n","        self.buffer.clear()\n","\n","    # -------------------------------------------------------\n","    def generate_contrafactual(self, what_if:str)->str:\n","        \"\"\"\n","        Resposta simples baseada no histórico + 'what_if' (ex.: 'e se\n","        a recompensa fosse dobrada?').\n","        \"\"\"\n","        if not self.buffer: return \"Sem dados.\"\n","        avg_r = sum(b[\"r\"] for b in self.buffer)/len(self.buffer)\n","        return (f\"Se '{what_if}', estimamos que a recompensa média \"\n","                f\"poderia mudar de {avg_r:.2f} para {avg_r*1.2:.2f}.\")"],"metadata":{"id":"SH9z6zKbzpHl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","integration_app.py\n","==================\n","• Junta LHRA_Agent (núcleo RL/autoevolução) +\n","  DeepSeek LLM (API) via LangChain +\n","  Gradio Chat UI com histórico.\n","\n","Executar em Colab/local:\n","------------------------------------------------\n","!pip install -q langchain~=0.1.0 gradio~=4.10.0 \\\n","                 sentence-transformers requests python-dotenv\n","# -> ou chame: pip install -r requirements.txt  (gerado abaixo)\n","\n","# coloque sua chave API (DeepSeek) em .env  (DEEPSEEK_API_KEY=...)\n","python integration_app.py\n","------------------------------------------------\n","\"\"\"\n","\n","# -------------------------  requirements.txt  --------------------------\n","REQUIREMENTS_TXT = \"\"\"\n","langchain==0.1.0\n","gradio==4.10.0\n","sentence-transformers==0.12.1\n","requests~=2.31.0\n","python-dotenv>=1.0.0\n","gymnasium~=0.29             # se for rodar ambientes do LHRA\n","\"\"\"\n","\n","# escrever requirements.txt se não existir\n","import os, textwrap, sys, json, time, threading, inspect\n","from pathlib import Path\n","requirements_file = Path(\"requirements.txt\")\n","if not requirements_file.exists():\n","    requirements_file.write_text(textwrap.dedent(REQUIREMENTS_TXT).strip()+\"\\n\")\n","\n","# -------------------------  DeepSeek LLM Wrapper  ----------------------\n","from langchain.llms.base import LLM\n","from typing import Any, Dict, List, Optional\n","import requests, dotenv, math\n","\n","dotenv.load_dotenv()\n","\n","class DeepSeekLLM(LLM):\n","    \"\"\"\n","    Wrapper simples para a API DeepSeek‑LLM.\n","    A API real pode diferir; adapte endpoint/payload conforme docs oficiais.\n","    \"\"\"\n","    model: str = \"deepseek-chat-v1\"        # id hipotético\n","    temperature: float = 0.7\n","    max_tokens: int = 512\n","    api_key: str = os.getenv(\"DEEPSEEK_API_KEY\", \"\")\n","\n","    @property\n","    def _llm_type(self) -> str:\n","        return \"deepseek-llm\"\n","\n","    # ------------- chamada principal ---------------\n","    def _call(\n","        self,\n","        prompt: str,\n","        stop: Optional[List[str]] = None,\n","        **kwargs: Any\n","    ) -> str:\n","        if not self.api_key:\n","            raise ValueError(\"Defina DEEPSEEK_API_KEY no .env!\")\n","\n","        url = \"https://api.deepseek.com/v1/completions\"  # endpoint fictício\n","        payload = {\n","            \"model\": self.model,\n","            \"prompt\": prompt,\n","            \"temperature\": self.temperature,\n","            \"max_tokens\": self.max_tokens,\n","        }\n","        if stop: payload[\"stop\"] = stop\n","\n","        headers = {\n","            \"Authorization\": f\"Bearer {self.api_key}\",\n","            \"Content-Type\": \"application/json\",\n","        }\n","        try:\n","            resp = requests.post(url, json=payload, headers=headers, timeout=60)\n","            resp.raise_for_status()\n","            data = resp.json()\n","            # formato fictício; ajuste ao real\n","            return data[\"choices\"][0][\"text\"].strip()\n","        except Exception as e:\n","            return f\"[DeepSeek API ERROR] {e}\"\n","\n","# -------------------  LHRA Agent (stub de integração)  -----------------\n","# Assumindo que as iterações 1‑6 já estão em lhra_core/.\n","try:\n","    sys.path.append(str(Path().absolute()))\n","    from lhra_core.agent import LHRA_Agent\n","except Exception as e:\n","    # Para quem não tem a implementação completa ainda,\n","    # gera um stub minimamente funcional.\n","    class LHRA_Agent:\n","        def __init__(self):\n","            self.state = \"[init]\"\n","            self.step_id = 0\n","        def step_user_input(self, user_msg:str) -> str:\n","            self.step_id += 1\n","            self.state = f\"s{self.step_id}\"\n","            # rascunho textual que será refinado pela LLM\n","            return f\"(internal‑state={self.state})\\nUser said: {user_msg}\"\n","\n","# -------------------  LangChain (Prompt & Chain) -----------------------\n","from langchain.prompts import PromptTemplate\n","from langchain.memory import ConversationBufferMemory\n","from langchain.chains import LLMChain\n","\n","prompt_template = PromptTemplate(\n","    input_variables=[\"chat_history\", \"lhra_draft\"],\n","    template=(\n","        \"Você é um assistente avançado que refina o rascunho do agente LHRA, \"\n","        \"melhorando clareza e precisão.\\n\"\n","        \"Histórico da conversa:\\n{chat_history}\\n\"\n","        \"Rascunho do LHRA:\\n{lhra_draft}\\n\"\n","        \"Resposta refinada:\"\n","    ),\n",")\n","\n","memory_chain = ConversationBufferMemory(memory_key=\"chat_history\",\n","                                         return_messages=True)\n","deepseek_llm = DeepSeekLLM()\n","chain = LLMChain(llm=deepseek_llm,\n","                 prompt=prompt_template,\n","                 memory=memory_chain)\n","\n","# ---------------------------  Gradio UI  -------------------------------\n","import gradio as gr\n","\n","agent = LHRA_Agent()         # núcleo\n","\n","def chat_with_agent(user_input, history):\n","    \"\"\"Função chamada pelo componente chat da UI.\"\"\"\n","    if not user_input: return \"\", history\n","\n","    # 1) LHRA gera rascunho\n","    lhra_draft = agent.step_user_input(user_input)\n","\n","    # 2) DeepSeek LLM refina\n","    refined = chain.run(lhra_draft=lhra_draft)\n","\n","    history = history + [(user_input, refined)]\n","    return \"\", history\n","\n","with gr.Blocks(title=\"LHRA + DeepSeek Chat\") as demo:\n","    gr.Markdown(\"### <center>🤖 LHRA&nbsp;Agent + DeepSeek&nbsp;LLM</center>\")\n","    chatbot = gr.Chatbot([], elem_id=\"chatbot\", height=450)\n","    msg = gr.Textbox(label=\"Digite aqui\")\n","    clear = gr.Button(\"Limpar\")\n","\n","    msg.submit(chat_with_agent, inputs=[msg, chatbot], outputs=[msg, chatbot])\n","    clear.click(lambda: ([], []), None, [chatbot])\n","\n","# ---------------------------  main  ------------------------------------\n","def main():\n","    # imprime help rápido\n","    print(\"🔑 Coloque sua chave DEEPSEEK_API_KEY em .env  (ou export).\")\n","    print(\"📦 Se faltar dependências, instale: pip install -r requirements.txt\")\n","    demo.launch()\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"5dY4oB_wzuKW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","lhra_monitor.py  (Iteração 8)\n","=============================\n","• Logger & Checkpointer para núcleo LHRA\n","• Painel TensorBoard (/runs/lhra)\n","• Endpoint Flask  /reload_checkpoint  -> carrega .pth mais recente\n","\"\"\"\n","\n","import os, time, json, psutil, threading, datetime, torch, queue\n","from pathlib import Path\n","from typing import Any, Dict\n","from torch.utils.tensorboard import SummaryWriter\n","from flask import Flask, request, jsonify\n","\n","# ---------- CONFIGURAÇÃO  ------------------\n","LOG_DIR        = Path(\"runs\") / \"lhra\"\n","CKPT_DIR       = Path(\"checkpoints\")\n","CKPT_DIR.mkdir(exist_ok=True, parents=True)\n","SAVE_EVERY_SEC = 120                         # checkpoint a cada 2 min\n","WRITE_EVERY    = 50                          # passos p/ TB summary\n","# ------------------------------------------\n","\n","class LHRA_Monitor:\n","    \"\"\" Monitora e faz checkpoint do agente em background. \"\"\"\n","    def __init__(self, agent, writer: SummaryWriter):\n","        self.agent     : Any   = agent\n","        self.writer    : SummaryWriter = writer\n","        self.last_save : float = 0.0\n","        self.step_idx  : int   = 0\n","        self.queue     : \"queue.Queue[Dict]\" = queue.Queue()\n","\n","        # inicia thread para salvar/checkpointer\n","        threading.Thread(target=self._loop, daemon=True).start()\n","\n","    # ------------ INTERFACE PÚBLICA ----------------\n","    def log_metrics(self, metrics: Dict[str, float]):\n","        \"\"\"Chamado pelo LHRA a cada step.\"\"\"\n","        self.queue.put(metrics)\n","\n","    # ------------ THREAD LOOP ----------------------\n","    def _loop(self):\n","        while True:\n","            try:\n","                metrics = self.queue.get(timeout=1.0)\n","                self._handle_metrics(metrics)\n","            except queue.Empty:\n","                pass\n","\n","            # checkpoint periódico\n","            if time.time() - self.last_save > SAVE_EVERY_SEC:\n","                self._save_checkpoint()\n","\n","    def _handle_metrics(self, m: Dict[str, float]):\n","        self.step_idx += 1\n","        if self.step_idx % WRITE_EVERY == 0:\n","            for k, v in m.items():\n","                self.writer.add_scalar(k, v, self.step_idx)\n","\n","            # recursos máquina\n","            self.writer.add_scalar(\"sys/cpu_pct\",\n","                                   psutil.cpu_percent(), self.step_idx)\n","            if psutil.sensors_temperatures():\n","                t = psutil.sensors_temperatures().get(\"coretemp\", [])[0].current\n","                self.writer.add_scalar(\"sys/cpu_temp\", t, self.step_idx)\n","\n","    def _save_checkpoint(self):\n","        ts   = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","        file = CKPT_DIR / f\"lhra_{ts}.pth\"\n","        try:\n","            torch.save({\n","                \"agent_state\": self.agent.state_dict(),\n","                \"step_idx\"   : self.step_idx,\n","                \"timestamp\"  : ts,\n","            }, file)\n","            print(f\"💾  Checkpoint salvo: {file}\")\n","            self.last_save = time.time()\n","        except Exception as e:\n","            print(f\"[CKPT‑ERROR] {e}\")\n","\n","# ---------------------------------------------------------------------\n","#  FLASK ‑ hot‑reload do checkpoint mais recente\n","# ---------------------------------------------------------------------\n","app = Flask(__name__)\n","GLOBAL_AGENT_REF: Any = None   # setado externamente\n","\n","@app.route(\"/reload_checkpoint\", methods=[\"POST\"])\n","def reload_ckpt():\n","    global GLOBAL_AGENT_REF\n","    try:\n","        ckpts = sorted(CKPT_DIR.glob(\"lhra_*.pth\"))\n","        if not ckpts:\n","            return jsonify({\"ok\": False, \"msg\": \"Nenhum checkpoint.\"}), 404\n","        latest = ckpts[-1]\n","        data   = torch.load(latest, map_location=\"cpu\")\n","        GLOBAL_AGENT_REF.load_state_dict(data[\"agent_state\"])\n","        return jsonify({\"ok\": True,\n","                        \"msg\": f\"Checkpoint {latest.name} carregado.\",\n","                        \"step\": data.get(\"step_idx\", -1)})\n","    except Exception as e:\n","        return jsonify({\"ok\": False, \"error\": str(e)}), 500\n","\n","def start_flask(port=5055):\n","    threading.Thread(target=lambda: app.run(\"0.0.0.0\", port),\n","                     daemon=True).start()\n","\n","# ---------------------------------------------------------------------\n","#                FUNÇÃO DE FÁCIL INTEGRAÇÃO\n","# ---------------------------------------------------------------------\n","def attach_monitor_to_agent(agent) -> LHRA_Monitor:\n","    \"\"\"Chamada de qualquer lugar (ex.: dentro de integration_app).\"\"\"\n","    writer  = SummaryWriter(LOG_DIR)\n","    mon     = LHRA_Monitor(agent, writer)\n","    # disponibiliza agente p/ endpoint\n","    global GLOBAL_AGENT_REF\n","    GLOBAL_AGENT_REF = agent\n","    start_flask()\n","    print(f\"📊 TensorBoard:  tensorboard --logdir={LOG_DIR}  (porta 6006)\")\n","    print(\"🔁  Endpoint reload: POST http://localhost:5055/reload_checkpoint\")\n","    return mon\n","\n","# ==================== TESTE RÁPIDO ==========================\n","if __name__ == \"__main__\":\n","    class Dummy:\n","        def state_dict(self):  return {\"x\": 1}\n","        def load_state_dict(self, d): print(\"state loaded\", d)\n","    ag  = Dummy()\n","    mon = attach_monitor_to_agent(ag)\n","    # simular métrica pingando\n","    for i in range(1000):\n","        mon.log_metrics({\"reward_avg\": math.sin(i/30)+1,\n","                         \"world_loss\": abs(math.cos(i/40))})\n","        time.sleep(0.05)"],"metadata":{"id":"5n_J1pNCz2Qn"},"execution_count":null,"outputs":[]}]}